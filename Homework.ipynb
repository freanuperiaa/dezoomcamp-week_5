{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f3c437-f063-491c-89b4-6bf796558deb",
   "metadata": {},
   "source": [
    "# Week 5 Homework\n",
    "\n",
    "#### Freanu Peria\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd053742-4cf4-4506-86a8-a99d70c32139",
   "metadata": {},
   "source": [
    "### For this homework we will be using the FHV 2019-10 data found here. FHV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58412331-3dcc-468a-9c32-9597025f2b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-04 20:25:56--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240304T122558Z&X-Amz-Expires=300&X-Amz-Signature=21c6ed11a15739c8908a307d1c826b75544fee29584689b0e3c2f7f701d68fab&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-03-04 20:25:57--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240304T122558Z&X-Amz-Expires=300&X-Amz-Signature=21c6ed11a15739c8908a307d1c826b75544fee29584689b0e3c2f7f701d68fab&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19375751 (18M) [application/octet-stream]\n",
      "Saving to: ‘fhv_tripdata_2019-10.csv.gz’\n",
      "\n",
      "fhv_tripdata_2019-1 100%[===================>]  18.48M  3.40MB/s    in 6.0s    \n",
      "\n",
      "2024-03-04 20:26:04 (3.09 MB/s) - ‘fhv_tripdata_2019-10.csv.gz’ saved [19375751/19375751]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf0d634-abc9-4980-8cdc-a6bd12ab5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the file inside data folder\n",
    "!mv fhv_tripdata_2019-10.csv.gz data/fhv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f66ae4-d93f-488f-892a-3e0c4cccf559",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "## Install Spark and PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7802490-dce9-4bc1-a9ea-621635fcc77f",
   "metadata": {},
   "source": [
    "\n",
    "\r\n",
    "Install Spark\r\n",
    "Run PySpark\r\n",
    "Create a local spark session\r\n",
    "Execute spark.version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1f62350-d215-4bc5-92b4-902e23706b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame.iteritems = pd.DataFrame.items # compatibility issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23ba7d16-0f3f-4f8e-bf52-f268e08e2c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/04 20:28:13 WARN Utils: Your hostname, coy-pc resolves to a loopback address: 127.0.1.1; using 172.19.138.153 instead (on interface eth0)\n",
      "24/03/04 20:28:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/04 20:28:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa26800d-52ca-459a-b466-7dc1a2f6bd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa6762-aa2f-4bb3-a153-ed12eb8ebf6e",
   "metadata": {},
   "source": [
    "### Question 1 Answer:\n",
    "\n",
    "3.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f19ed6-4169-4d93-9a45-f3a0743c379e",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "## FHV October 2019\n",
    "\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons\n",
    "\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet\n",
    "\n",
    "### What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122214d-22ac-4867-8f0b-a93c2d0e95ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to get the schema of fhv dataset. we can get the schema from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38d3cc18-af3d-4cd2-af64-033681f1c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_fhv_df = pd.read_csv('data/fhv/fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c77e4bb5-2400-4b29-b136-733e5c668c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2019-10-01 00:23:00</td>\n",
       "      <td>2019-10-01 00:35:00</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00013</td>\n",
       "      <td>2019-10-01 00:11:29</td>\n",
       "      <td>2019-10-01 00:13:22</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:11:43</td>\n",
       "      <td>2019-10-01 00:37:20</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:56:29</td>\n",
       "      <td>2019-10-01 00:57:47</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:23:09</td>\n",
       "      <td>2019-10-01 00:28:27</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0               B00009  2019-10-01 00:23:00  2019-10-01 00:35:00   \n",
       "1               B00013  2019-10-01 00:11:29  2019-10-01 00:13:22   \n",
       "2               B00014  2019-10-01 00:11:43  2019-10-01 00:37:20   \n",
       "3               B00014  2019-10-01 00:56:29  2019-10-01 00:57:47   \n",
       "4               B00014  2019-10-01 00:23:09  2019-10-01 00:28:27   \n",
       "\n",
       "   PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0         264.0         264.0      NaN                 B00009  \n",
       "1         264.0         264.0      NaN                 B00013  \n",
       "2         264.0         264.0      NaN                 B00014  \n",
       "3         264.0         264.0      NaN                 B00014  \n",
       "4         264.0         264.0      NaN                 B00014  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_fhv_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80047eb7-a12f-4631-a498-d2c10765a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark df from pd_fhv_df and get schema\n",
    "# spark.createDataFrame(pd_fhv_df).schema\n",
    "# tunrs out converting the pandas df to spark df causes an error because of the mixed dtypes that pandas have accepted.\n",
    "\n",
    "# let's just create the schema\n",
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('PUlocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOlocationID', types.IntegerType(), True), \n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23b113d8-5ebb-47a6-914d-d829ee3debda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_df = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .schema(schema) \\\n",
    "    .csv('data/fhv/fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "993dd305-7037-4de9-ac77-a276ad3d454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   null|                B00014|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fhv_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ed9d01b-98d9-42d7-bc3e-4b54f6655352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition\n",
    "fhv_df = fhv_df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3350104-e233-4db0-af29-85a661597f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fhv_df.write.parquet(\"data/pq/fhv/201910\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3800a787-ee27-45c7-ba8d-fad2ab643c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 39M\n",
      "-rw-r--r-- 1 frean frean    0 Mar  4 20:42 _SUCCESS\n",
      "-rw-r--r-- 1 frean frean 6.4M Mar  4 20:42 part-00000-9e3158f7-5cb3-44e0-9c8a-148b790a492a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 frean frean 6.4M Mar  4 20:42 part-00001-9e3158f7-5cb3-44e0-9c8a-148b790a492a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 frean frean 6.4M Mar  4 20:42 part-00002-9e3158f7-5cb3-44e0-9c8a-148b790a492a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 frean frean 6.4M Mar  4 20:42 part-00003-9e3158f7-5cb3-44e0-9c8a-148b790a492a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 frean frean 6.4M Mar  4 20:42 part-00004-9e3158f7-5cb3-44e0-9c8a-148b790a492a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 frean frean 6.4M Mar  4 20:42 part-00005-9e3158f7-5cb3-44e0-9c8a-148b790a492a-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/pq/fhv/201910"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e2041-d978-4bf5-a7e3-77177736f38d",
   "metadata": {},
   "source": [
    "### Question 2 Answer:\n",
    "\n",
    "6.4Mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0217d323-35cb-4aee-8d57-8b30f1640ffb",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "## Count records\n",
    "\n",
    "##### How many taxi trips were there on the 15th of October?\n",
    "\n",
    "##### Consider only trips that started on the 15th of October."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cf2fb46-769a-409f-8689-d14c2dbda0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02111|2019-10-27 05:35:21|2019-10-27 05:38:04|          92|          80|   null|                B02111|\n",
      "|              B01437|2019-10-08 09:41:27|2019-10-08 09:46:52|         264|         197|   null|                B01437|\n",
      "|              B02107|2019-10-09 16:53:55|2019-10-09 17:06:05|         264|         167|   null|                B02107|\n",
      "|              B01653|2019-10-15 06:47:13|2019-10-15 06:59:36|         264|         264|   null|                B01653|\n",
      "|              B00850|2019-10-13 03:57:31|2019-10-13 04:12:36|         264|         177|   null|                B00850|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fhv_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0cb6379-c597-4fde-9820-9610eddf4a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would like to use SQL on this.\n",
    "# so I will be making a view on our spark session\n",
    "\n",
    "fhv_df.createOrReplaceTempView('fhv_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58e4d623-5f14-4f03-9f8b-97fc6489a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "oct_trips_res = spark.sql(\"\"\"\n",
    "    SELECT COUNT(1)\n",
    "    FROM fhv_data\n",
    "    WHERE pickup_datetime >= '2019-10-15 00:00:00'\n",
    "    and dropoff_datetime <= '2019-10-15 23:59:59'\n",
    "    --and dropoff_datetime <= '2019-10-16'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98a86e3a-31d3-45b4-aa16-e442eb13a981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   61851|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "oct_trips_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c589c-81ef-4bd0-b63c-ee6f7fd3994c",
   "metadata": {},
   "source": [
    "### Question 3 Answer:\n",
    "\n",
    "62, 610 (closest answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a74180-43aa-4461-83a3-eb833272d1e6",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "## Longest trip for each day\n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6fb28fc1-f06e-440e-9d31-292d9d1d5daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 133:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|      trip_duration|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------------+\n",
      "|              B02832|2019-10-11 18:00:00|2091-10-11 18:30:00|         264|         264|   null|                B02832|          -631152.5|\n",
      "|              B02832|2019-10-28 09:00:00|2091-10-28 09:30:00|         264|         264|   null|                B02832|          -631152.5|\n",
      "|              B02416|2019-10-31 23:46:33|2029-11-01 00:13:00|        null|        null|   null|                B02416| -87672.44083333333|\n",
      "|     B00746         |2019-10-01 21:43:42|2027-10-01 21:45:23|         159|         264|   null|       B00746         | -70128.02805555555|\n",
      "|              B02921|2019-10-17 14:00:00|2020-10-18 00:00:00|        null|        null|   null|                B03037|            -8794.0|\n",
      "|              B03110|2019-10-26 21:26:00|2020-10-26 21:36:00|         264|         264|   null|                B03110| -8784.166666666666|\n",
      "|              B03080|2019-10-30 12:30:04|2019-12-30 13:02:08|         264|          50|   null|                B02883|-1464.5344444444445|\n",
      "|     B03084         |2019-10-25 07:04:57|2019-12-08 07:54:33|         168|         235|   null|                B02765|-1056.8266666666666|\n",
      "|     B03084         |2019-10-25 07:04:57|2019-12-08 07:21:11|         168|         235|   null|                B02765|-1056.2705555555556|\n",
      "|              B01452|2019-10-01 13:47:17|2019-11-03 15:20:28|          44|         214|   null|                B01452| -793.5530555555556|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# let's try to add a new column which calculates difference of pickup from dropoff\n",
    "# good thing i found a solution on how to subtract timestamps\n",
    "# from https://sparkbyexamples.com/pyspark/pyspark-timestamp-difference-seconds-minutes-hours/\n",
    "\n",
    "fhv_df.withColumn(\n",
    "    'trip_duration', \n",
    "    (F.col('pickup_datetime').cast('long') - F.col('dropoff_datetime').cast('long')) / 3600\n",
    "    ) \\\n",
    "    .orderBy('trip_duration') \\\n",
    "    .show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6d9ee2-47ec-4def-9fc8-afa0a98a3496",
   "metadata": {},
   "source": [
    "### Question 4 Answer\n",
    "\n",
    "631,152.5 Hours (^First row in above, with dispatching_base_num=B02832)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5b2f62-ddec-4b65-ace2-8c82a3ea4132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af02b9bc-c475-4163-b931-5d22615ee0ff",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "## User Interface\n",
    "\n",
    "Spark’s User Interface which shows the application's dashboard runs on which local port?\r\n",
    "Answer: 4040\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f941f3ff-5963-4725-972f-e35651b48ae1",
   "metadata": {},
   "source": [
    "##  Question 6:\n",
    "## Least frequent pickup location zone\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6cfb9bf3-f6cf-4457-9443-4826717c7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is already taxi_zone_lookup. we can read it in our data.\n",
    "\n",
    "zones_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5dfca2f5-4841-4875-a881-a082f99f4f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zones_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ccd7ecda-3d20-467d-9319-d71a7b42d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would like to use SQL again :D\n",
    "# lets create another view\n",
    "zones_df.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2aa44d3a-416b-4aad-878b-96db5b42088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "least_frequent_zones_df = spark.sql(\"\"\"\n",
    "\n",
    "    SELECT COUNT(1), b.zone\n",
    "    FROM fhv_data a\n",
    "        JOIN zones b\n",
    "        ON a.PUlocationID = b.locationID\n",
    "    GROUP BY b.zone\n",
    "    ORDER BY COUNT(1)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e05fef89-c3cc-4555-af0a-7fbf095f345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 155:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|count(1)|                zone|\n",
      "+--------+--------------------+\n",
      "|       1|         Jamaica Bay|\n",
      "|       2|Governor's Island...|\n",
      "|       5| Green-Wood Cemetery|\n",
      "|       8|       Broad Channel|\n",
      "|      14|     Highbridge Park|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "least_frequent_zones_df.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9924e-11d7-4611-9e04-7db9743633f1",
   "metadata": {},
   "source": [
    "### Question 6 Answer\n",
    "\n",
    "Jamaica Bay\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db344b14-c6b7-4b26-9f34-cc015f985d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
